{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pacific-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from object.loss import CrossEntropyLabelSmooth, Entropy\n",
    "from object import network\n",
    "import os, sys\n",
    "import os.path as osp\n",
    "os.environ['CUDA_VISILE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "novel-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_copy(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr0'] = param_group['lr']\n",
    "    return optimizer\n",
    "\n",
    "def lr_scheduler(optimizer, iter_num, max_iter, gamma=10, power=0.75):\n",
    "    decay = (1 + gamma * iter_num / max_iter) ** (-power)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr0'] * decay\n",
    "        param_group['weight_decay'] = 1e-3\n",
    "        param_group['momentum'] = 0.9\n",
    "        param_group['nesterov'] = True\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confident-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageList(Dataset):\n",
    "    def __init__(self, imgs_path, transform, mode='RGB'):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.imgs_path[idx].split(',')\n",
    "        img = Image.open(path).convert(self.mode)\n",
    "        return self.transform(img), int(label)\n",
    "    \n",
    "def train_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def test_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def load_data(source_path, target_path, bsize):\n",
    "    dsets = {}\n",
    "    dloaders = {}\n",
    "    \n",
    "    src_txt = open(source_path, 'r').readlines()\n",
    "    target_txt = open(target_path, 'r').readlines()\n",
    "    \n",
    "    dsize = len(src_txt)\n",
    "    train_size = int(0.9 * dsize)\n",
    "    \n",
    "    train_txt, val_txt = torch.utils.data.random_split(src_txt, [train_size, dsize - train_size])\n",
    "    \n",
    "    dsets['source_train'] = ImageList(train_txt, transform=train_transform())\n",
    "    dloaders['source_train'] = DataLoader(dsets['source_train'], batch_size=bsize, shuffle=True, drop_last=False)\n",
    "    \n",
    "    dsets['source_val'] = ImageList(val_txt, transform=test_transform())\n",
    "    dloaders['source_val'] = DataLoader(dsets['source_val'], batch_size=bsize, shuffle=True, drop_last=False)\n",
    "    \n",
    "    dsets['target_train'] = ImageList(target_txt, transform=train_transform())\n",
    "    dloaders['target_train'] = DataLoader(dsets['target_train'], batch_size=bsize*2, shuffle=True, drop_last=False)\n",
    "\n",
    "    \n",
    "    dsets['target_test'] = ImageList(target_txt, transform=test_transform())\n",
    "    dloaders['target_test'] = DataLoader(dsets['target_test'], batch_size=bsize*2, shuffle=True, drop_last=False)\n",
    "    \n",
    "    return dsets, dloaders\n",
    "\n",
    "def cal_acc(loader, netF, netB, netC):\n",
    "    netF.eval()\n",
    "    netB.eval()\n",
    "    netC.eval()\n",
    "    \n",
    "    pred, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.cuda()\n",
    "            output = netC(netB(netF(x)))\n",
    "            pred.append(output.float().cpu())\n",
    "            true.append(y.float())\n",
    "\n",
    "    pred, true = torch.cat(pred), torch.cat(true)\n",
    "    pred = nn.Softmax(dim=1)(pred)\n",
    "    _, pred = torch.max(pred, 1)\n",
    "    acc = (torch.squeeze(pred).float() == true).float().mean()\n",
    "    return acc.item()\n",
    "\n",
    "def source_train(dloaders):\n",
    "    netF = network.ResBase(res_name='resnet50').cuda()\n",
    "    netB = network.feat_bootleneck(type='bn', feature_dim=netF.in_features, bottleneck_dim=256).cuda()\n",
    "    netC = network.feat_classifier(type='wn', class_num=65, bottleneck_dim=256).cuda()\n",
    "    \n",
    "    param_group = []\n",
    "    learning_rate = 1e-2\n",
    "    for k, v in netF.named_parameters():\n",
    "        param_group += [{'params': v, 'lr': learning_rate*0.1}]\n",
    "    for k, v in netB.named_parameters():\n",
    "        param_group += [{'params': v, 'lr': learning_rate}]\n",
    "    for k, v in netC.named_parameters():\n",
    "        param_group += [{'params': v, 'lr': learning_rate}]   \n",
    "    optimizer = optim.SGD(param_group)\n",
    "    optimizer = op_copy(optimizer)\n",
    "    \n",
    "    max_iter = 20 * len(dloaders['source_train'])\n",
    "    \n",
    "    netF.train()\n",
    "    netB.train()\n",
    "    netC.train()\n",
    "    \n",
    "    best_acc = 0\n",
    "    bestF, bestB, bestC = None, None, None\n",
    "    \n",
    "    for iter_num in range(max_iter):\n",
    "        total_loss = 0\n",
    "        total_length = 0\n",
    "        for i, (source_x, source_y) in enumerate(dloaders['source_train']):\n",
    "            lr_scheduler(optimizer, iter_num=iter_num, max_iter=max_iter)\n",
    "            source_x, source_y = source_x.cuda(), source_y.cuda()\n",
    "            \n",
    "            outputs = netC(netB(netF(source_x)))\n",
    "            loss = CrossEntropyLabelSmooth(num_classes=65, epsilon=0.1)(outputs, source_y)\n",
    "            \n",
    "            total_loss += len(source_x)*loss.item()\n",
    "            total_length += len(source_x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print('Step: %02d/%02d, Training Loss: %.4f' % (i+1, len(dloaders['source_train']), total_loss / total_length), end='\\r')\n",
    "\n",
    "        acc_val = cal_acc(dloaders['source_val'], netF, netB, netC)\n",
    "        netF.train()\n",
    "        netB.train()\n",
    "        netC.train()\n",
    "        print('Iter: %03d/%03d, Valid Acc: %.2f%%' % (iter_num + 1, max_iter, 100*acc_val))\n",
    "        \n",
    "        if acc_val > best_acc:\n",
    "            best_acc = acc_val\n",
    "            bestF, bestB, bestC = netF, netB, netC \n",
    "\n",
    "    torch.save(bestF.state_dict(), './model/OfficeHome/source_F.pt')\n",
    "    torch.save(bestB.state_dict(), './model/OfficeHome/source_B.pt')\n",
    "    torch.save(bestC.state_dict(), './model/OfficeHome/source_C.pt')\n",
    "\n",
    "    return bestF, bestB, bestC\n",
    "    \n",
    "def target_train(dloaders, netF, netB, netC):\n",
    "    netF.train()\n",
    "    netB.train()\n",
    "    netC.eval()\n",
    "    \n",
    "    for k, v in netC.named_parameters():\n",
    "        v.requires_grad = False\n",
    "    \n",
    "    param_group = []\n",
    "    learning_rate = 1e-2\n",
    "    for k, v in netF.named_parameters():\n",
    "        param_group += [{'params': v, 'lr': learning_rate*0.1}]\n",
    "    for k, v in netB.named_parameters():\n",
    "        param_group += [{'params': v, 'lr': learning_rate}]\n",
    "        \n",
    "    optimizer = optim.SGD(param_group)\n",
    "    optimizer = op_copy(optimizer)\n",
    "    \n",
    "    max_iter = 20\n",
    "    \n",
    "    for iter_num in range(max_iter):\n",
    "        for i, (target_x, target_y) in enumerate(dloaders['target_train']):\n",
    "            lr_scheduler(optimizer, iter_num=iter_num, max_iter=max_iter)\n",
    "            target_x, target_y = target_x.cuda(), target_y.cuda()\n",
    "            \n",
    "            features = netB(netF(target_x))\n",
    "            output = netC(features)\n",
    "            \n",
    "            softmax_output = nn.Softmax(dim=1)(output)\n",
    "            entropy_loss = torch.mean(Entropy(softmax_output))\n",
    "            \n",
    "            msoftmax = softmax_output.mean(dim=0)\n",
    "            entropy_loss -= torch.sum(-msoftmax * torch.log(msoftmax + 1e-5))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            entropy_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print('Iter: %02d, Step: %02d/%02d' % (iter_num+1, i+1, len(dloaders['target_train'])), end='\\r')\n",
    "#         acc = cal_acc(dloaders['target_test'], netF, netB, netC)\n",
    "#         netF.train()\n",
    "#         netB.train()\n",
    "#         print('Iter: %03d/%03d, Valid Acc: %.2f%%' % (iter_num + 1, max_iter, 100*acc))\n",
    "        \n",
    "    torch.save(netF.state_dict(), './model/OfficeHome/target_F.pt')\n",
    "    torch.save(netB.state_dict(), './model/OfficeHome/target_B.pt')\n",
    "    torch.save(netC.state_dict(), './model/OfficeHome/target_C.pt')\n",
    "    \n",
    "    return netF, netB, netC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composed-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_path(path, name):\n",
    "    res = ''\n",
    "    path = osp.join(path, name)\n",
    "    for i, sub_forder in enumerate(sorted(os.listdir(path))):\n",
    "        for file in sorted(os.listdir(osp.join(path, sub_forder))):\n",
    "            res += osp.join(path, sub_forder, file) + ',%d\\n' % (i)\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "underlying-engineer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    names = ['Art', 'Clipart', 'Product', 'RealWorld']\n",
    "    class_num = 65\n",
    "\n",
    "    data_forder = '../data/OfficeHome'\n",
    "    train_bs = 32\n",
    "\n",
    "    source_path = osp.join(data_forder, names[0] + '.txt')\n",
    "    target_path = osp.join(data_forder, names[1] + '.txt')\n",
    "\n",
    "    dsets, dloaders = load_data(source_path, target_path, train_bs)\n",
    "    \n",
    "    netF = network.ResBase(res_name='resnet50').cuda()\n",
    "    netB = network.feat_bootleneck(type='bn', feature_dim=netF.in_features, bottleneck_dim=256).cuda()\n",
    "    netC = network.feat_classifier(type='wn', class_num=65, bottleneck_dim=256).cuda()\n",
    "    \n",
    "    netF.load_state_dict(torch.load('./model/OfficeHome/target_F.pt'))\n",
    "    netB.load_state_dict(torch.load('./model/OfficeHome/target_B.pt'))\n",
    "    netC.load_state_dict(torch.load('./model/OfficeHome/target_C.pt'))\n",
    "    print('Accuracy: %.2f%%' % (100*cal_acc(dloaders['target_test'], netF, netB, netC)))\n",
    "    \n",
    "#     netF, netB, netC = target_train(dloaders, netF, netB, netC)\n",
    "\n",
    "#     netF, netB, netC = source_train(dloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "promotional-tenant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.85%\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
